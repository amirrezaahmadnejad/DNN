{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-Deep Learning with python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnrvwrW7vaF4Y0dyqQJMyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirrezaahmadnejad/DNN/blob/master/1_Deep_Learning_with_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCi2XQvaTUn6",
        "colab_type": "text"
      },
      "source": [
        "Learning, in the context of machine learning, describes an automatic search\n",
        "process for better representations.Machine-learning algorithms aren’t usually creative in finding these transformations; they’re merely searching through a predefined set of operations, called a hypothesis space.\n",
        "\n",
        "what deep learning is, technically: a multistage way to learn data representations. It’s a simple idea—but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic.\n",
        "![](https://drive.google.com/open?id=1huRRemQQ76xDQpHtRka25H86kphkd9cW)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugd_g3AxWCMM",
        "colab_type": "text"
      },
      "source": [
        "**learn to classify handwritten digits**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoVZ5zDeV4mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14.0 #Down grade tensorflow beacue now it is on 2 version\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61yQx4F9W6Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HevX2XvmXIHa",
        "colab_type": "text"
      },
      "source": [
        "The images are encoded as Numpy arrays, and the labels are an array of digits, ranging from 0 to 9. The images and labels have a one-to-one correspondence.\n",
        "Now what we do:\n",
        "First, we’ll feed the neural network the training data, train_images and train_labels. The network will then learn to associate images and labels. Finally, we’ll ask the network to produce predictions for test_images, and we’ll\n",
        "verify whether these predictions match the labels from test_labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXNnmU9FXvxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2BB5x2SYGo9",
        "colab_type": "text"
      },
      "source": [
        "our network consists of a sequence of two Dense layers, which are densely\n",
        "connected (also called fully connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
        "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
        "\n",
        "\n",
        "*   **A loss function**—How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
        "*   **An optimizer**—The mechanism through which the network will update itself\n",
        "based on the data it sees and its loss function.\n",
        "*   **Metrics** to monitor during training and testing—Here, we’ll only care about accuracy (the fraction of the images that were correctly classified).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vuv4WsDYzD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfXKX5TqZNyq",
        "colab_type": "text"
      },
      "source": [
        "Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLzJEGF5Y2wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4FhDwUGZVfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# categorically encode the labels\n",
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuqOGFs9ZnYJ",
        "colab_type": "text"
      },
      "source": [
        "Ready to train the network, which in Keras is done via a call to the network’s fit method—we fit the model to its training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "essN_3elZhGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwkKM1TaEzc",
        "colab_type": "text"
      },
      "source": [
        "Two quantities are displayed during training: **the loss of the network over the training data**, and **the accuracy of the network over the training data.**\n",
        "Now let’s check that the model performs well on the test set, too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAWsJO-fZssZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk9PBW3gaeW6",
        "colab_type": "text"
      },
      "source": [
        "This gap between training accuracy and test accuracy is an example of\n",
        "***overfitting***: \n",
        "\n",
        "\"the fact that machine-learning models tend to perform worse on new data than on their training data.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bon8tLiatgo",
        "colab_type": "text"
      },
      "source": [
        "What’s a tensor?\n",
        "A tensor is a container for data\n",
        "\n",
        "\n",
        "\n",
        "*  **Scalars (0D tensors):**\n",
        "\n",
        "A tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional tensor, or 0D tensor). In the context of tensors, a dimension is often called an axis.\n",
        "\n",
        "In Numpy, a float32 or float64 number is a scalar tensor (or scalar array).\n",
        "You can display the number of axes of a Numpy tensor via the *ndim* attribute; a scalar tensor has 0 axes (ndim == 0). The number of axes of a tensor is also called its rank.\n",
        "\n",
        "```\n",
        "x = np.array(12)\n",
        "```\n",
        "*  **Vectors (1D tensors):**\n",
        "\n",
        "An array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis. \n",
        "\n",
        "```\n",
        "x = np.array([12, 3, 6, 14])\n",
        "```\n",
        "This vector has five entries and so is called a 5-dimensional vector. Don’t confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis). Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times.\n",
        "it’s technically more correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes)\n",
        "\n",
        "\n",
        "*   **Matrices (2D tensors)**\n",
        "\n",
        "An array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to rows and columns). \n",
        "\n",
        "\n",
        "```\n",
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "```\n",
        "\n",
        "\n",
        "*   **3D tensors and higher-dimensional tensors**\n",
        "\n",
        "If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers.\n",
        "\n",
        "\n",
        "```\n",
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]]])\n",
        "```\n",
        "A tensor is defined by three key attributes:\n",
        "\n",
        "\n",
        "1.   *Number of axes (rank):*a 3D tensor has three axes, and a matrix has\n",
        "two axes. This is also called the tensor’s ndim in Python libraries such as Numpy\n",
        "2.   *Shape:*This is a tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape\n",
        "(3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape\n",
        "with a single element, such as (5,), whereas a scalar has an empty shape, ().\n",
        "3.   *Data type:*This is the type of the data\n",
        "contained in the tensor; for instance, a tensor’s type could be float32, uint8,\n",
        "float64, and so on.**String tensors don’t exist in Numpy**\n",
        "\n",
        "For example for train:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        ">>> print(train_images.ndim)\n",
        "3\n",
        ">>> print(train_images.shape)\n",
        "(60000, 28, 28)\n",
        ">>> print(train_images.dtype)\n",
        "uint8\n",
        "```\n",
        "So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of 60,000 matrices of 28 × 8 integers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYBAtaT-pTEr",
        "colab_type": "text"
      },
      "source": [
        "Selecting specific elements in a tensor is called tensor slicing.\n",
        "The following example selects digits #10 to #100 (#100 isn’t included) and puts\n",
        "them in an array of shape (90, 28, 28):\n",
        "\n",
        "\n",
        "```\n",
        ">>> my_slice = train_images[10:100]\n",
        ">>> print(my_slice.shape)\n",
        "(90, 28, 28)\n",
        "```\n",
        "It’s equivalent to this more detailed notation, which specifies a start index and stop index for the slice along each tensor axis. Note that : is equivalent to selecting the entire axis:\n",
        "\n",
        "\n",
        "```\n",
        ">>> my_slice = train_images[10:100, :, :]\n",
        ">>> my_slice.shape\n",
        "(90, 28, 28)\n",
        ">>> my_slice = train_images[10:100, 0:28, 0:28]\n",
        ">>> my_slice.shape\n",
        "(90, 28, 28)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1_Y93F8q-zc",
        "colab_type": "text"
      },
      "source": [
        "In our initial example, we were building our network by stacking Dense layers on top of each other. A Keras layer instance looks like this:\n",
        "\n",
        "\n",
        "```\n",
        "keras.layers.Dense(512, activation='relu')\n",
        "```\n",
        "This layer can be interpreted as a function, which takes as input a 2D tensor and\n",
        "returns another 2D tensor—a new representation for the input tensor. Specifically, the\n",
        "function is as follows (where W is a 2D tensor and b is a vector, both attributes of the\n",
        "layer):\n",
        "\n",
        "\n",
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```\n",
        "We have three tensor operations here: a dot product (dot) between\n",
        "the input tensor and a tensor named W; an addition (+) between the resulting 2D tensor and a vector b; and, finally, a relu operation. relu(x) is max(x, 0).\n",
        "\n",
        "\n",
        "\n",
        "1.   The relu operation and addition are element-wise operations: operations that are applied independently to each entry in the tensors being considered. \n",
        "\n",
        "**As naive Python implementation of an element-wise operation:**\n",
        "\n",
        "*   an element-wise relu operation:\n",
        "\n",
        "\n",
        "```\n",
        "def naive_relu(x):\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "  assert len(x.shape) == 2\n",
        "  x = x.copy() #Do not use original one\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i, j], 0)\n",
        "  return x\n",
        "```\n",
        "\n",
        "\n",
        "*   Addition:\n",
        "\n",
        "\n",
        "```\n",
        "def naive_add(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "  x = x.copy() #Do not use original one\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x\n",
        "```\n",
        "when dealing with Numpy arrays, these operations are available as welloptimized built-in Numpy functions, which themselves delegate the heavy lifting to a\n",
        "Basic Linear Algebra Subprograms (BLAS) implementation if you have one installedwhen dealing with Numpy arrays, these operations are available as welloptimized built-in Numpy functions, which themselves delegate the heavy lifting to a\n",
        "Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "z = x + y\n",
        "z = np.maximum(z, 0.)\n",
        "```\n",
        "2.   Broadcasting\n",
        "\n",
        "In the Dense layer introduced earlier, we added a 2D\n",
        "tensor with a vector. What happens with addition when the shapes of the two tensors\n",
        "being added differ?\n",
        "\n",
        "\n",
        "When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to\n",
        "match the shape of the larger tensor. Broadcasting consists of two steps:\n",
        "\n",
        "\n",
        "*   Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
        "the larger tensor.\n",
        "*   The smaller tensor is repeated alongside these new axes to match the full shape\n",
        "of the larger tensor.\n",
        "\n",
        "Consider X with shape (32, 10) and y with shape\n",
        "(10,). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we\n",
        "repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape\n",
        "(32, 10), where Y[i, :] == y for i in range(0, 32). At this point, we can proceed to\n",
        "add X and Y, because they have the same shape.\n",
        "\n",
        "In terms of implementation, no new 2D tensor is created, because that would be\n",
        "terribly inefficient. The repetition operation is entirely virtual.Here’s what a naive\n",
        "implementation would look like:\n",
        "\n",
        "\n",
        "```\n",
        "def naive_add_matrix_and_vector(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  x = x.copy()\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[j]\n",
        "  return x\n",
        "```\n",
        "But in Numpy:\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)\n",
        "```\n",
        "3.   Tensor dot\n",
        "\n",
        "The dot operation, also called a tensor product is the most common, most useful tensor operation. Contrary to element-wise operations, it combines entries in the input tensors.\n",
        "\n",
        "An element-wise product is done with the * operator\n",
        "\n",
        "dot uses a different syntax in TensorFlow, but in both Numpy and\n",
        "Keras it’s done using the standard dot operator:\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "z = np.dot(x, y)\n",
        "```\n",
        "![image.png](data:image/png;base64,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)\n",
        "You can also take the dot product between a matrix x and a vector y, which returns\n",
        "a vector where the coefficients are the dot products between y and the rows of x.\n",
        "\n",
        "As navie:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "def naive_matrix_vector_dot(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 1\n",
        "  assert x.shape[1] == y.shape[0]\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      z[i] += x[i, j] * y[j]\n",
        "  return z\n",
        "```\n",
        "In Numpy\n",
        "```\n",
        "def naive_matrix_vector_dot(x, y):\n",
        "  z = np.zeros(x.shape[0])\n",
        "  for i in range(x.shape[0]):\n",
        "    z[i] = naive_vector_dot(x[i, :], y)\n",
        "  return z\n",
        "```\n",
        "Note that as soon as one of the two tensors has an ndim greater than 1, dot is no longer symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x).\n",
        "\n",
        "More generally, you can take the dot product between higher-dimensional tensors,\n",
        "following the same rules for shape compatibility as outlined earlier for the 2D case:\n",
        "(a, b, c, d) . (d,) -> (a, b, c)\n",
        "(a, b, c, d) . (d, e) -> (a, b, c, e)\n",
        "\n",
        "4.   Tensor reshaping\n",
        "\n",
        "Reshaping a tensor means rearranging its rows and columns to match a target shape.\n",
        "Naturally, the reshaped tensor has the same total number of coefficients as the initial\n",
        "tensor. \n",
        "```\n",
        ">>> x = np.array([[0., 1.],\n",
        "                  [2., 3.],\n",
        "                  [4., 5.]])\n",
        ">>> print(x.shape)\n",
        "(3, 2)\n",
        ">>> x = x.reshape((6, 1))\n",
        ">>> x\n",
        "array([[ 0.],\n",
        "      [ 1.],\n",
        "      [ 2.],\n",
        "      [ 3.],\n",
        "      [ 4.],\n",
        "      [ 5.]])\n",
        ">>> x = x.reshape((2, 3))\n",
        ">>> x\n",
        "array([[ 0., 1., 2.],\n",
        "      [ 3., 4., 5.]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItMurbQBIOr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```\n",
        "W and b are tensors that are attributes of the layer. They’re called\n",
        "the weights or trainable parameters of the layer (the kernel and bias attributes, respectively)\n",
        "Initially, these weight matrices are filled with small random values (a step called random initialization).What comes next is to gradually\n",
        "adjust these weights, based on a feedback signal. This gradual adjustment, also called\n",
        "training, is basically the learning that machine learning is all about.\n",
        "\n",
        "training loop, which works as follows. Repeat\n",
        "these steps in a loop, as long as necessary:\n",
        "# Mini-batch stochastic gradient descent (minibatch SGD)\n",
        "1.   Draw a batch of training samples x and corresponding targets y.\n",
        "2.   Run the network on x (a step called the forward pass) to obtain predictions y_pred.\n",
        "3.   Compute the loss of the network on the batch, a measure of the mismatch\n",
        "between y_pred and y.\n",
        "4.   Compute the gradient of the loss with regard to the network’s parameters (a\n",
        "backward pass).\n",
        "5. Move the parameters a little in the opposite direction from the gradient—for\n",
        "example W -= step * gradient—thus reducing the loss on the batch a bit.\n",
        "\n",
        "#Optimizers\n",
        "There exist multiple variants of SGD that differ by taking into account\n",
        "previous weight updates when computing the next weight update, rather than just\n",
        "looking at the current value of the gradients.\n",
        "There is, for instance, SGD with momentum. Such variants are known as optimization methods or optimizers. \n",
        "\n",
        "Momentum addresses two issues with\n",
        "SGD: convergence speed and local minima.\n",
        "Momentum is implemented by moving\n",
        "the ball at each step based not only on the current slope value (current acceleration)\n",
        "but also on the current velocity (resulting from past acceleration). In practice, this\n",
        "means updating the parameter w based not only on the current gradient value but also\n",
        "on the previous parameter update.\n",
        "\n",
        "\n",
        "```\n",
        "past_velocity = 0.\n",
        "momentum = 0.1\n",
        "while loss > 0.01:\n",
        "w, loss, gradient = get_current_parameters()\n",
        "velocity = past_velocity * momentum + learning_rate * gradient\n",
        "w = w + momentum * velocity - learning_rate * gradient\n",
        "past_velocity = velocity\n",
        "update_parameter(w)\n",
        "```\n",
        "#Backpropagation algorithm\n",
        "In practice, a neural network function\n",
        "consists of many tensor operations chained together, each of which has a simple,\n",
        "known derivative. For instance, this is a network f composed of three tensor operations, a, b, and c, with weight matrices W1, W2, and W3:\n",
        "\n",
        "f(W1, W2, W3) = a(W1, b(W2, c(W3)))\n",
        "a chain of functions can be derived using the following identity, called the chain rule: f(g(x)) = f'(g(x)) * g'(x). Applying the chain rule to the\n",
        "computation of the gradient values of a neural network gives rise to an algorithm called Backpropagation.\n",
        "\n",
        " Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter\n",
        "had in the loss value.\n",
        "\n",
        "Thanks to symbolic differentiation, you’ll\n",
        "never have to implement the Backpropagation algorithm by hand\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPuUyidTHCIT",
        "colab_type": "text"
      },
      "source": [
        "#**Summery**\n",
        "This was the input data:\n",
        "\n",
        "\n",
        "```\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "```\n",
        "the input images are stored in Numpy tensors, which are\n",
        "here formatted as float32 tensors of shape (60000, 784) (training data) and (10000,\n",
        "784) (test data), respectively.\n",
        "\n",
        "This was our network:\n",
        "\n",
        "\n",
        "```\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "```\n",
        "this network consists of a chain of two Dense layers, that\n",
        "each layer applies a few simple tensor operations to the input data, and that these\n",
        "operations involve weight tensors. Weight tensors, which are attributes of the layers,\n",
        "are where the knowledge of the network persists.\n",
        "\n",
        "This was the network-compilation step:\n",
        "\n",
        "\n",
        "```\n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "```\n",
        "categorical_crossentropy is the loss function that’s used\n",
        "as a feedback signal for learning the weight tensors, and which the training phase will\n",
        "attempt to minimize. You also know that this reduction of the loss happens via minibatch stochastic gradient descent. The exact rules governing a specific use of gradient\n",
        "descent are defined by the rmsprop optimizer passed as the first argument.\n",
        "\n",
        "Finally, this was the training loop:\n",
        "\n",
        "\n",
        "```\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "```\n",
        "when you call fit: the network will start to iterate\n",
        "on the training data in mini-batches of 128 samples, 5 times over (each iteration over\n",
        "all the training data is called an epoch). At each iteration, the network will compute the\n",
        "gradients of the weights with regard to the loss on the batch, and update the weights accordingly.\n",
        "\n",
        "\n"
      ]
    }
  ]
}