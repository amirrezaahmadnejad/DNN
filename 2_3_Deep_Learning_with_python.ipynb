{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.3-Deep Learning with python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG9odi_ELcNR",
        "colab_type": "text"
      },
      "source": [
        "#**Predicting house prices: a regression example**\n",
        "\n",
        "Another common type of\n",
        "machine-learning problem is regression, which consists of predicting a continuous\n",
        "value instead of a discrete label: for instance, predicting the temperature tomorrow,\n",
        "given meteorological data; or predicting the time that a software project will take to\n",
        "complete, given its specifications.\n",
        "You’ll attempt to predict the median price of homes in a given Boston suburb in the\n",
        "mid-1970s, given data points about the suburb at the time, such as the crime rate, the\n",
        "local property tax rate, and so on.\n",
        "\n",
        "Dataset has relatively few data points: only 506, split\n",
        "between 404 training samples and 102 test samples. And each feature in the input data\n",
        "(for example, the crime rate) has a different scale.\n",
        "For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others between 0 and 100, and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzmh_iyOLSu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b760adb-e38a-439c-e4b9-d001207c1467"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) =boston_housing.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGPX19ioOi_8",
        "colab_type": "text"
      },
      "source": [
        "It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous\n",
        "data, but it would definitely make learning more difficult. A widespread best practice\n",
        "to deal with such data is to do **feature-wise normalization**:\n",
        "\n",
        "for each feature in the input\n",
        "data (a column in the input data matrix), you subtract the mean of the feature and\n",
        "divide by the standard deviation, so that the feature is centered around 0 and has a\n",
        "unit standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZXNGFp6Om_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2uZwcsnO0ZJ",
        "colab_type": "text"
      },
      "source": [
        "**You should never use in your workflow any quantity computed on the\n",
        "test data, even for something as simple as data normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcR42s72O80o",
        "colab_type": "text"
      },
      "source": [
        "Because so few samples are available, you’ll use a very small network with two hidden\n",
        "layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXhzFlrYOxNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',\n",
        "                          input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn5X5at0Prpt",
        "colab_type": "text"
      },
      "source": [
        "The network ends with a single unit and no activation (it will be a linear layer). This is\n",
        "a typical setup for scalar regression (a regression where you’re trying to predict a single\n",
        "continuous value)\n",
        "you compile the network with the mse loss function—mean squared error,\n",
        "the square of the difference between the predictions and the targets. This is a widely\n",
        "used loss function for regression problems.\n",
        "You’re also monitoring a new metric during training: mean absolute error (MAE). It’s\n",
        "the absolute value of the difference between the predictions and the targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnA0LlZiQM9v",
        "colab_type": "text"
      },
      "source": [
        "**Validiation beacuse eof small size!!!**\n",
        "\n",
        "To evaluate your network while you keep adjusting its parameters (such as the number\n",
        "of epochs used for training), you could split the data into a training set and a validation set, as you did in the previous examples. But because you have so few data points,\n",
        "the validation set would end up being very small (for instance, about 100 examples).\n",
        "\n",
        "The best practice in such situations is to use K-fold cross-validation\n",
        "![alt text](https://www.researchgate.net/profile/B_Aksasse/publication/326866871/figure/fig2/AS:669601385947145@1536656819574/K-fold-cross-validation-In-addition-we-outline-an-overview-of-the-different-metrics-used.jpg)\n",
        "\n",
        "It consists of splitting the available data into K partitions (typically K = 4 or 5), instantiating K identical models, and training each one on K – 1 partitions while evaluating on\n",
        "the remaining partition. The validation score for the model used is then the average of\n",
        "the K validation scores obtained. In terms of code, this is straightforward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AITSQzJpRMMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNgZ0f8RNft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(4):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate([train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0)\n",
        "  partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,epochs=100, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Xtzr-STbFf",
        "colab_type": "text"
      },
      "source": [
        "**Summery:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Regression is done using different loss functions than what we used for classification. Mean squared error (MSE) is a loss function commonly used for regression.\n",
        "\n",
        "2.   Evaluation metrics to be used for regression differ from those used for\n",
        "classification; naturally, the concept of accuracy doesn’t apply for regression. A\n",
        "common regression metric is mean absolute error (MAE).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RafupzqdTdzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}